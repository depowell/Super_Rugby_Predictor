{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# comprehend list for years\n",
    "years = [str(2000 + i) for i in range(5,19)]\n",
    "this_year = '2019'\n",
    "print(years)\n",
    "\n",
    "# where do we get the data?\n",
    "current_year_url = 'http://www.superrugby.co.nz/Grandstand'\n",
    "url = 'http://www.superrugby.co.nz/Grandstand/HistoricalResults/' # year appends here\n",
    "\n",
    "# getter function\n",
    "def get_rugby_data(url, year):\n",
    "    '''getting data from super rugby website'''\n",
    "    if year == this_year:\n",
    "        x = ''\n",
    "    else:\n",
    "        x = year\n",
    "    page = requests.get(url + x)\n",
    "    soup = bs(page.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all previous years data: run this once\n",
    "for i in years:\n",
    "    data = get_rugby_data(url, i)\n",
    "    f = open(\"data/data_\" + i + \".txt\",\"w+\")\n",
    "    f.write(str(data))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get this years data, run this after every round\n",
    "data = get_rugby_data(current_year_url, this_year)\n",
    "f = open(\"data/data_\" + this_year + \".txt\",\"w+\")\n",
    "f.write(str(data))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple functions for cleaning data\n",
    "# regex for finding round names\n",
    "pattern = re.compile(\"^(Round|Week|Semifinal|Final|Qualifiers|Semis)(\\ \\d{1,2})?.*$\")\n",
    "\n",
    "def parse_date(date):\n",
    "    date = dt.datetime.strptime(date, '%d %b %Y')\n",
    "    return date\n",
    "\n",
    "def outcome(f):\n",
    "    '''game outcome for home team V: victory L: loss D: draw'''\n",
    "    if f > 0:\n",
    "        return 'V'\n",
    "    elif f < 0:\n",
    "        return 'L'\n",
    "    elif f == 0:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'D'\n",
    "\n",
    "def fix_round(f):\n",
    "    '''extract round number or final type'''\n",
    "    if f[:4] == 'Week':\n",
    "        return f[5:7]\n",
    "    elif f[:5] == 'Round':\n",
    "        return f[6:8]\n",
    "    elif f[:10] == 'Qualifiers' or f[:13] == 'Quarterfinals':\n",
    "        return 'QF' # quarter final\n",
    "    elif f[:6] == 'Finals' or f == 'Semifinals' or f == 'Semis' or f == 'Semifinal':\n",
    "        return 'SF' # semi final\n",
    "    elif f[:6] == 'Final ' or f == 'Final':\n",
    "        return 'GF' # grand final\n",
    "    else:\n",
    "        return f\n",
    "    \n",
    "def data_nice(year):\n",
    "    table_nice = []\n",
    "    table_round = []\n",
    "    with open('data/data_' + year + '.txt') as f:\n",
    "        data = bs(f.read())\n",
    "    rows = data.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols_nice = [ele.text.strip() for ele in cols]\n",
    "        cols_round = [x.text.strip() for x in cols if pattern.match(x.text.strip())]\n",
    "        table_nice.append([ele for ele in cols_nice if ele]) # Get rid of empty values\n",
    "        table_round.append([ele for ele in cols_round if ele]) # Get rid of empty values\n",
    "    df1 = pd.DataFrame(table_nice)\n",
    "    df2 = pd.DataFrame(table_round).fillna(method='ffill')\n",
    "    df = pd.concat([df1, df2], axis=1).dropna()\n",
    "    df['year'] = year\n",
    "    df.columns = ['date','teams','location','time','score','round','year']\n",
    "    df['date'] = df['date'] + ' ' + df['year']\n",
    "    df['home'] = df['teams'].str.split(' v ').str[0]\n",
    "    #df['location'] = [clean_location(x) for x in df['location']]\n",
    "    df['away'] = df['teams'].str.split(' v ').str[1]\n",
    "    df['home'] = df['home'].str.strip()\n",
    "    df['away'] = df['away'].str.strip()\n",
    "    df['hp'] = df['score'].str.split('-').str[0].astype('int') # home points\n",
    "    df['ap'] = df['score'].str.split('-').str[1].astype('int') # away points\n",
    "    df['sm'] = df['hp'] - df['ap'] # score margin\n",
    "    df['ho'] = [outcome(x) for x in df['sm']] # home outcome\n",
    "    df['round'] = [fix_round(x) for x in df['round']]\n",
    "    remove_columns = ['teams','score','year']\n",
    "    df = df.drop(columns=remove_columns)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframes, cleaning up data:\n",
    "\n",
    "df_2005 = data_nice('2005')\n",
    "df_2006 = data_nice('2006')\n",
    "df_2007 = data_nice('2007')\n",
    "df_2008 = data_nice('2008')\n",
    "df_2009 = data_nice('2009')\n",
    "df_2010 = data_nice('2010')\n",
    "df_2011 = data_nice('2011')\n",
    "df_2012 = data_nice('2012')\n",
    "df_2013 = data_nice('2013')\n",
    "df_2014 = data_nice('2014')\n",
    "df_2015 = data_nice('2015')\n",
    "df_2016 = data_nice('2016')\n",
    "df_2017 = data_nice('2017')\n",
    "df_2018 = data_nice('2018')\n",
    "df_2019 = data_nice('2019')\n",
    "\n",
    "# more fixing data inconsistancies\n",
    "df_2005.loc[(df_2005['date'] == '28 May 2005'), 'round'] = \"GF\" # 2005 no final fixed\n",
    "df_2006.drop(5, inplace=True) # remove bogus final data from 2006\n",
    "df_2018.drop(10, inplace=True) # remove bogus final data from 2018\n",
    "# List of series missing from each year\n",
    "missing_games_2007 = [pd.Series(['12 May 2007', 'Durban', '3:00 PM', 'SF',\n",
    "                           'Sharks', 'Blues', \n",
    "                           34, 18, 16, 'V'], index=df_2007.columns ) ,\n",
    "                      pd.Series(['12 May 2007', 'Pretoria', '5:30 PM', 'SF', \n",
    "                           'Bulls', 'Crusaders', \n",
    "                           27, 12, 15, 'V'], index=df_2007.columns )]\n",
    "\n",
    "missing_games_2008 = [pd.Series(['31 May 2008', 'AMI Stadium, Christchurch', '7:35 PM', 'GF',\n",
    "                           'Crusaders', 'Waratahs', \n",
    "                           20, 12, 8, 'V'], index=df_2008.columns ) ,\n",
    "                      pd.Series(['24 May 2008', 'Sydney', '9:40 PM', 'SF', \n",
    "                           'Waratahs', 'Sharks', \n",
    "                           28, 13, 15, 'V'], index=df_2008.columns ),\n",
    "                      pd.Series(['24 May 2008', 'AMI Stadium, Christchurch', '7:35 PM', 'SF', \n",
    "                           'Crusaders', 'Hurricanes', \n",
    "                           33, 22, 11, 'V'], index=df_2008.columns )]\n",
    "\n",
    "missing_games_2017 = [pd.Series(['21 Jul 2017', 'Canberra', '9:35 PM', 'QF',\n",
    "                           'Brumbies', 'Hurricanes', \n",
    "                           16, 35, -19, 'L'], index=df_2017.columns ) ,\n",
    "                      \n",
    "                      pd.Series(['22 Jul 2017', 'AMI Stadium, Christchurch', '7:35 PM', 'QF', \n",
    "                           'Crusaders', 'Highlanders', \n",
    "                           17, 0, 17, 'V'], index=df_2017.columns ),\n",
    "                      \n",
    "                      pd.Series(['23 Jul 2017', 'Johannesburg', '12:30 AM', 'QF', \n",
    "                           'Lions', 'Sharks', \n",
    "                           23, 21, 2, 'V'], index=df_2017.columns ),\n",
    "                      \n",
    "                      pd.Series(['23 Jul 2017', 'Cape Town', '3:00 AM', 'QF', \n",
    "                           'Stormers', 'Chiefs', \n",
    "                           11, 17, -6, 'L'], index=df_2017.columns )]\n",
    "\n",
    "# Pass a list of series to the append() to add multiple rows to 2007\n",
    "df_2007 = df_2007.append(missing_games_2007 , ignore_index=True)\n",
    "df_2008 = df_2008.append(missing_games_2008 , ignore_index=True)\n",
    "df_2017 = df_2017.append(missing_games_2017 , ignore_index=True)\n",
    "\n",
    "\n",
    "df_2009.at[6, 'home'] = 'Chiefs'\n",
    "df_2009.at[7, 'home'] = 'Bulls'\n",
    "df_2009.at[8, 'home'] = 'Bulls'\n",
    "\n",
    "df_2010.at[4, 'home'] = 'Bulls'\n",
    "\n",
    "df_2013.at[2, 'home'] = 'Crusaders'\n",
    "df_2013.at[3, 'home'] = 'Brumbies'\n",
    "df_2013.at[4, 'home'] = 'Chiefs'\n",
    "df_2013.at[5, 'home'] = 'Bulls'\n",
    "df_2013.at[6, 'home'] = 'Chiefs'\n",
    "\n",
    "df_2014.at[6, 'round'] = 'GF'\n",
    "df_2014.at[2, 'round'] = 'QF'\n",
    "df_2014.at[3, 'round'] = 'QF'\n",
    "\n",
    "df_2015.at[2, 'round'] = 'QF'\n",
    "df_2015.at[3, 'round'] = 'QF'\n",
    "df_2015.at[6, 'round'] = 'GF'\n",
    "\n",
    "df_2016.at[2, 'round'] = 'QF'\n",
    "df_2016.at[3, 'round'] = 'QF'\n",
    "df_2016.at[4, 'round'] = 'QF'\n",
    "df_2016.at[5, 'round'] = 'QF'\n",
    "df_2016.at[8, 'round'] = 'GF'\n",
    "\n",
    "df_2016.at[6, 'home'] = 'Crusaders'\n",
    "df_2016.at[7, 'home'] = 'Lions'\n",
    "\n",
    "df_2018.at[152, 'round'] = 'QF'\n",
    "df_2018.at[153, 'round'] = 'QF'\n",
    "df_2018.at[154, 'round'] = 'QF'\n",
    "df_2018.at[155, 'round'] = 'QF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse dates and sort, reset indexes\n",
    "df_2005.date = df_2005.date.apply(parse_date)\n",
    "df_2006.date = df_2006.date.apply(parse_date)\n",
    "df_2007.date = df_2007.date.apply(parse_date)\n",
    "df_2008.date = df_2008.date.apply(parse_date)\n",
    "df_2009.date = df_2009.date.apply(parse_date)\n",
    "df_2010.date = df_2010.date.apply(parse_date)\n",
    "df_2011.date = df_2011.date.apply(parse_date)\n",
    "df_2012.date = df_2012.date.apply(parse_date)\n",
    "df_2013.date = df_2013.date.apply(parse_date)\n",
    "df_2014.date = df_2014.date.apply(parse_date)\n",
    "df_2015.date = df_2015.date.apply(parse_date)\n",
    "df_2016.date = df_2016.date.apply(parse_date)\n",
    "df_2017.date = df_2017.date.apply(parse_date)\n",
    "df_2018.date = df_2018.date.apply(parse_date)\n",
    "\n",
    "# reset indexes\n",
    "df_2005 = df_2005.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2006 = df_2006.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2007 = df_2007.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2008 = df_2008.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2009 = df_2009.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2010 = df_2010.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2011 = df_2011.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2012 = df_2012.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2013 = df_2013.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2014 = df_2014.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2015 = df_2015.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2016 = df_2016.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2017 = df_2017.sort_values(by=['date']).reset_index(drop=True)\n",
    "df_2018 = df_2018.sort_values(by=['date']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>time</th>\n",
       "      <th>round</th>\n",
       "      <th>home</th>\n",
       "      <th>away</th>\n",
       "      <th>hp</th>\n",
       "      <th>ap</th>\n",
       "      <th>sm</th>\n",
       "      <th>ho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>Cape Town</td>\n",
       "      <td>2:05 AM</td>\n",
       "      <td>1</td>\n",
       "      <td>Stormers</td>\n",
       "      <td>Jaguares</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>Johannesburg</td>\n",
       "      <td>4:15 AM</td>\n",
       "      <td>1</td>\n",
       "      <td>Lions</td>\n",
       "      <td>Sharks</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-23</td>\n",
       "      <td>Forsyth Barr Stadium, Dunedin</td>\n",
       "      <td>7:35 PM</td>\n",
       "      <td>2</td>\n",
       "      <td>Highlanders</td>\n",
       "      <td>Blues</td>\n",
       "      <td>41</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-23</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>9:45 PM</td>\n",
       "      <td>2</td>\n",
       "      <td>Rebels</td>\n",
       "      <td>Reds</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-24</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>5:15 PM</td>\n",
       "      <td>2</td>\n",
       "      <td>Sunwolves</td>\n",
       "      <td>Brumbies</td>\n",
       "      <td>25</td>\n",
       "      <td>32</td>\n",
       "      <td>-7</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                       location     time round         home  \\\n",
       "0 2018-02-18                      Cape Town  2:05 AM    1      Stormers   \n",
       "1 2018-02-18                   Johannesburg  4:15 AM    1         Lions   \n",
       "2 2018-02-23  Forsyth Barr Stadium, Dunedin  7:35 PM     2  Highlanders   \n",
       "3 2018-02-23                      Melbourne  9:45 PM     2       Rebels   \n",
       "4 2018-02-24                          Tokyo  5:15 PM     2    Sunwolves   \n",
       "\n",
       "       away  hp  ap  sm ho  \n",
       "0  Jaguares  28  20   8  V  \n",
       "1    Sharks  26  19   7  V  \n",
       "2     Blues  41  34   7  V  \n",
       "3      Reds  45  19  26  V  \n",
       "4  Brumbies  25  32  -7  L  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_2018.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
